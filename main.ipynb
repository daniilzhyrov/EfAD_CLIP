{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVTec LOCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class MVTecDataset(Dataset):\n",
    "    def __init__(self, root_dir, category=\"all\", phase=\"train\", transform=None, anomaly_types=None, clip_transform=None):\n",
    "        \"\"\"\n",
    "        root_dir (string): Directory with all the images.\n",
    "        category (string): Category of images ('all' or any of the specific categories).\n",
    "        phase (string): One of 'train', 'validation', or 'test'.\n",
    "        transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        anomaly_type (string, optional): Specifies the type of anomaly for the test phase. \n",
    "                                          Can be 'good', 'logical_anomalies', or 'structural_anomalies'.\n",
    "                                          This parameter is ignored if phase is not 'test'.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.phase = phase\n",
    "        self.anomaly_types = anomaly_types\n",
    "        self.categories = [category] if category != \"all\" else ['breakfast_box', 'juice_bottle', 'pushpins', 'screw_bag', 'splicing_connectors']\n",
    "        self.data = []\n",
    "        self.clip_transform = clip_transform\n",
    "        self._load_dataset()\n",
    "\n",
    "    def _load_dataset(self):\n",
    "        for category in self.categories:\n",
    "            if self.phase in ['train', 'validation']:\n",
    "                category_path = os.path.join(self.root_dir, category, self.phase, 'good')\n",
    "                for img_name in os.listdir(category_path):\n",
    "                    if img_name.endswith('.png'):\n",
    "                        self.data.append(os.path.join(category_path, img_name))\n",
    "            elif self.phase == 'combined_validation':\n",
    "                category_path = os.path.join(self.root_dir, category, 'validation', 'good')\n",
    "                for img_name in os.listdir(category_path):\n",
    "                    if img_name.endswith('.png'):\n",
    "                        self.data.append(os.path.join(category_path, img_name))\n",
    "                category_path = os.path.join(self.root_dir, category, 'test', 'good')\n",
    "                for img_name in os.listdir(category_path):\n",
    "                    if img_name.endswith('.png'):\n",
    "                        self.data.append(os.path.join(category_path, img_name))\n",
    "            elif self.phase == 'test':\n",
    "                anomaly_list = ['good', 'logical_anomalies', 'structural_anomalies']\n",
    "                if self.anomaly_types is not None:\n",
    "                    anomaly_list = self.anomaly_types\n",
    "                for anomaly_type in anomaly_list:\n",
    "                    category_path = os.path.join(self.root_dir, category, self.phase, anomaly_type)\n",
    "                    if os.path.exists(category_path):\n",
    "                        for img_name in os.listdir(category_path):\n",
    "                            if img_name.endswith('.png'):\n",
    "                                self.data.append((os.path.join(category_path, img_name), anomaly_type))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.phase == 'test':\n",
    "            img_path, anomaly_type = self.data[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            gt_mask = torch.zeros(256, 256)\n",
    "            if anomaly_type != 'good':\n",
    "                mask_path = os.path.join(img_path.replace('test', 'ground_truth').replace('.png', ''), '000.png')\n",
    "                mask = Image.open(mask_path).convert('L')\n",
    "                if self.transform:\n",
    "                    gt_mask = self.transform(mask)\n",
    "                gt_mask = gt_mask.squeeze()\n",
    "            if self.clip_transform:\n",
    "                clip_image = Image.open(img_path).convert('RGB')\n",
    "                clip_image = self.clip_transform(clip_image)\n",
    "                return {\n",
    "                    'image': image,\n",
    "                    'gt_mask': gt_mask,\n",
    "                    'anomaly_type': anomaly_type,\n",
    "                    'clip_image': clip_image\n",
    "                }\n",
    "            return {\n",
    "                    'image': image,\n",
    "                    'gt_mask': gt_mask,\n",
    "                    'anomaly_type': anomaly_type,\n",
    "                }\n",
    "                \n",
    "        else:\n",
    "            img_path = self.data[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagenet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class ImagenetDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        root_dir (string): Directory with all the images.\n",
    "        category (string): Category of images ('all' or any of the specific categories).\n",
    "        phase (string): One of 'train', 'validation', or 'test'.\n",
    "        transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        anomaly_type (string, optional): Specifies the type of anomaly for the test phase. \n",
    "                                          Can be 'good', 'logical_anomalies', or 'structural_anomalies'.\n",
    "                                          This parameter is ignored if phase is not 'test'.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self._load_dataset()\n",
    "\n",
    "    def _load_dataset(self):\n",
    "        train_path = os.path.join(self.root_dir, 'train')\n",
    "        for category in os.listdir(train_path):\n",
    "            if os.path.isdir(os.path.join(train_path, category)):\n",
    "                category_path = os.path.join(train_path, category)\n",
    "                for img_name in os.listdir(category_path):\n",
    "                    if img_name.endswith('.JPEG'):\n",
    "                        self.data.append(os.path.join(category_path, img_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (256, 256)\n",
    "\n",
    "# Dataset and DataLoader setup\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "data_transforms_imagenet = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size[0] * 2, image_size[1] * 2)),\n",
    "        transforms.RandomGrayscale(p=0.3),\n",
    "        transforms.CenterCrop((image_size[0], image_size[1])),\n",
    "        transforms.ToTensor(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "train_dataset = MVTecDataset(root_dir='mvtec_loco_anomaly_detection', category='breakfast_box', phase='train', transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validation_dataset = MVTecDataset(root_dir='mvtec_loco_anomaly_detection', category='breakfast_box', phase='validation', transform=transform)\n",
    "combined_validation_dataset = MVTecDataset(root_dir='mvtec_loco_anomaly_detection', category='breakfast_box', phase='combined_validation', transform=transform)\n",
    "validation_loader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "combined_validation_loader = DataLoader(dataset=combined_validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = MVTecDataset(root_dir='mvtec_loco_anomaly_detection', category='breakfast_box', phase='test', transform=transform)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "imagenet_dataset = ImagenetDataset(root_dir='imagenette2', transform=data_transforms_imagenet)\n",
    "imagenet_loader = DataLoader(dataset=imagenet_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EffecientAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Torch model for student, teacher and autoencoder model in EfficientAd.\"\"\"\n",
    "\n",
    "import logging\n",
    "import math\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F  # noqa: N812\n",
    "from torchvision import transforms\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def imagenet_norm_batch(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Normalize batch of images with ImageNet mean and std.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input batch.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized batch using the ImageNet mean and std.\n",
    "    \"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])[None, :, None, None].to(x.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])[None, :, None, None].to(x.device)\n",
    "    return (x - mean) / std\n",
    "\n",
    "\n",
    "def reduce_tensor_elems(tensor: torch.Tensor, m: int = 2**24) -> torch.Tensor:\n",
    "    \"\"\"Reduce tensor elements.\n",
    "\n",
    "    This function flatten n-dimensional tensors,  selects m elements from it\n",
    "    and returns the selected elements as tensor. It is used to select\n",
    "    at most 2**24 for torch.quantile operation, as it is the maximum\n",
    "    supported number of elements.\n",
    "    https://github.com/pytorch/pytorch/blob/b9f81a483a7879cd3709fd26bcec5f1ee33577e6/aten/src/ATen/native/Sorting.cpp#L291.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): input tensor from which elements are selected\n",
    "        m (int): number of maximum tensor elements.\n",
    "            Defaults to ``2**24``\n",
    "\n",
    "    Returns:\n",
    "            Tensor: reduced tensor\n",
    "    \"\"\"\n",
    "    tensor = torch.flatten(tensor)\n",
    "    if len(tensor) > m:\n",
    "        # select a random subset with m elements.\n",
    "        perm = torch.randperm(len(tensor), device=tensor.device)\n",
    "        idx = perm[:m]\n",
    "        tensor = tensor[idx]\n",
    "    return tensor\n",
    "\n",
    "\n",
    "class EfficientAdModelSize(str, Enum):\n",
    "    \"\"\"Supported EfficientAd model sizes.\"\"\"\n",
    "\n",
    "    M = \"medium\"\n",
    "    S = \"small\"\n",
    "\n",
    "\n",
    "class SmallPatchDescriptionNetwork(nn.Module):\n",
    "    \"\"\"Patch Description Network small.\n",
    "\n",
    "    Args:\n",
    "        out_channels (int): number of convolution output channels\n",
    "        padding (bool): use padding in convoluional layers\n",
    "            Defaults to ``False``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_channels: int, padding: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        pad_mult = 1 if padding else 0\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=4, stride=1, padding=3 * pad_mult)\n",
    "        self.conv2 = nn.Conv2d(128, 256, kernel_size=4, stride=1, padding=3 * pad_mult)\n",
    "        self.conv3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1 * pad_mult)\n",
    "        self.conv4 = nn.Conv2d(256, out_channels, kernel_size=4, stride=1, padding=0 * pad_mult)\n",
    "        self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=2, padding=1 * pad_mult)\n",
    "        self.avgpool2 = nn.AvgPool2d(kernel_size=2, stride=2, padding=1 * pad_mult)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output from the network.\n",
    "        \"\"\"\n",
    "        x = imagenet_norm_batch(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.avgpool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.avgpool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        return self.conv4(x)\n",
    "\n",
    "\n",
    "class MediumPatchDescriptionNetwork(nn.Module):\n",
    "    \"\"\"Patch Description Network medium.\n",
    "\n",
    "    Args:\n",
    "        out_channels (int): number of convolution output channels\n",
    "        padding (bool): use padding in convoluional layers\n",
    "            Defaults to ``False``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_channels: int, padding: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        pad_mult = 1 if padding else 0\n",
    "        self.conv1 = nn.Conv2d(3, 256, kernel_size=4, stride=1, padding=3 * pad_mult)\n",
    "        self.conv2 = nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=3 * pad_mult)\n",
    "        self.conv3 = nn.Conv2d(512, 512, kernel_size=1, stride=1, padding=0 * pad_mult)\n",
    "        self.conv4 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1 * pad_mult)\n",
    "        self.conv5 = nn.Conv2d(512, out_channels, kernel_size=4, stride=1, padding=0 * pad_mult)\n",
    "        self.conv6 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding=0 * pad_mult)\n",
    "        self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=2, padding=1 * pad_mult)\n",
    "        self.avgpool2 = nn.AvgPool2d(kernel_size=2, stride=2, padding=1 * pad_mult)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output from the network.\n",
    "        \"\"\"\n",
    "        x = imagenet_norm_batch(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.avgpool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.avgpool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        return self.conv6(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Autoencoder Encoder model.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.enconv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.enconv2 = nn.Conv2d(32, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.enconv3 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.enconv4 = nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.enconv5 = nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.enconv6 = nn.Conv2d(64, 64, kernel_size=8, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Perform the forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output from the network.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.enconv1(x))\n",
    "        x = F.relu(self.enconv2(x))\n",
    "        x = F.relu(self.enconv3(x))\n",
    "        x = F.relu(self.enconv4(x))\n",
    "        x = F.relu(self.enconv5(x))\n",
    "        return self.enconv6(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Autoencoder Decoder model.\n",
    "\n",
    "    Args:\n",
    "        out_channels (int): number of convolution output channels\n",
    "        padding (int): use padding in convoluional layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_channels: int, padding: int, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.padding = padding\n",
    "        # use ceil to match output shape of PDN\n",
    "        self.deconv1 = nn.Conv2d(64, 64, kernel_size=4, stride=1, padding=2)\n",
    "        self.deconv2 = nn.Conv2d(64, 64, kernel_size=4, stride=1, padding=2)\n",
    "        self.deconv3 = nn.Conv2d(64, 64, kernel_size=4, stride=1, padding=2)\n",
    "        self.deconv4 = nn.Conv2d(64, 64, kernel_size=4, stride=1, padding=2)\n",
    "        self.deconv5 = nn.Conv2d(64, 64, kernel_size=4, stride=1, padding=2)\n",
    "        self.deconv6 = nn.Conv2d(64, 64, kernel_size=4, stride=1, padding=2)\n",
    "        self.deconv7 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv8 = nn.Conv2d(64, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.dropout3 = nn.Dropout(p=0.2)\n",
    "        self.dropout4 = nn.Dropout(p=0.2)\n",
    "        self.dropout5 = nn.Dropout(p=0.2)\n",
    "        self.dropout6 = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, image_size: tuple[int, int] | torch.Size) -> torch.Tensor:\n",
    "        \"\"\"Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input batch.\n",
    "            image_size (tuple): size of input images.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output from the network.\n",
    "        \"\"\"\n",
    "        last_upsample = (\n",
    "            math.ceil(image_size[0] / 4) if self.padding else math.ceil(image_size[0] / 4) - 8,\n",
    "            math.ceil(image_size[1] / 4) if self.padding else math.ceil(image_size[1] / 4) - 8,\n",
    "        )\n",
    "        x = F.interpolate(x, size=(image_size[0] // 64 - 1, image_size[1] // 64 - 1), mode=\"bilinear\")\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.interpolate(x, size=(image_size[0] // 32, image_size[1] // 32), mode=\"bilinear\")\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.interpolate(x, size=(image_size[0] // 16 - 1, image_size[1] // 16 - 1), mode=\"bilinear\")\n",
    "        x = F.relu(self.deconv3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.interpolate(x, size=(image_size[0] // 8, image_size[1] // 8), mode=\"bilinear\")\n",
    "        x = F.relu(self.deconv4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = F.interpolate(x, size=(image_size[0] // 4 - 1, image_size[1] // 4 - 1), mode=\"bilinear\")\n",
    "        x = F.relu(self.deconv5(x))\n",
    "        x = self.dropout5(x)\n",
    "        x = F.interpolate(x, size=(image_size[0] // 2 - 1, image_size[1] // 2 - 1), mode=\"bilinear\")\n",
    "        x = F.relu(self.deconv6(x))\n",
    "        x = self.dropout6(x)\n",
    "        x = F.interpolate(x, size=last_upsample, mode=\"bilinear\")\n",
    "        x = F.relu(self.deconv7(x))\n",
    "        return self.deconv8(x)\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    \"\"\"EfficientAd Autoencoder.\n",
    "\n",
    "    Args:\n",
    "       out_channels (int): number of convolution output channels\n",
    "       padding (int): use padding in convoluional layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_channels: int, padding: int, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(out_channels, padding)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, image_size: tuple[int, int] | torch.Size) -> torch.Tensor:\n",
    "        \"\"\"Perform the forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input batch.\n",
    "            image_size (tuple): size of input images.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output from the network.\n",
    "        \"\"\"\n",
    "        x = imagenet_norm_batch(x)\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(x, image_size)\n",
    "\n",
    "\n",
    "class EfficientAdModel(nn.Module):\n",
    "    \"\"\"EfficientAd model.\n",
    "\n",
    "    Args:\n",
    "        teacher_out_channels (int): number of convolution output channels of the pre-trained teacher model\n",
    "        model_size (str): size of student and teacher model\n",
    "        padding (bool): use padding in convoluional layers\n",
    "            Defaults to ``False``.\n",
    "        pad_maps (bool): relevant if padding is set to False. In this case, pad_maps = True pads the\n",
    "            output anomaly maps so that their size matches the size in the padding = True case.\n",
    "            Defaults to ``True``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        teacher_out_channels: int,\n",
    "        model_size: EfficientAdModelSize = EfficientAdModelSize.S,\n",
    "        padding: bool = False,\n",
    "        pad_maps: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.pad_maps = pad_maps\n",
    "        self.teacher: MediumPatchDescriptionNetwork | SmallPatchDescriptionNetwork\n",
    "        self.student: MediumPatchDescriptionNetwork | SmallPatchDescriptionNetwork\n",
    "\n",
    "        if model_size == EfficientAdModelSize.M:\n",
    "            self.teacher = MediumPatchDescriptionNetwork(out_channels=teacher_out_channels, padding=padding).eval()\n",
    "            self.student = MediumPatchDescriptionNetwork(out_channels=teacher_out_channels * 2, padding=padding)\n",
    "\n",
    "        elif model_size == EfficientAdModelSize.S:\n",
    "            self.teacher = SmallPatchDescriptionNetwork(out_channels=teacher_out_channels, padding=padding).eval()\n",
    "            self.student = SmallPatchDescriptionNetwork(out_channels=teacher_out_channels * 2, padding=padding)\n",
    "\n",
    "        else:\n",
    "            msg = f\"Unknown model size {model_size}\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        self.ae: AutoEncoder = AutoEncoder(out_channels=teacher_out_channels, padding=padding)\n",
    "        self.teacher_out_channels: int = teacher_out_channels\n",
    "\n",
    "        self.mean_std: nn.ParameterDict = nn.ParameterDict(\n",
    "            {\n",
    "                \"mean\": torch.zeros((1, self.teacher_out_channels, 1, 1)),\n",
    "                \"std\": torch.zeros((1, self.teacher_out_channels, 1, 1)),\n",
    "            },\n",
    "        )\n",
    "\n",
    "        self.quantiles: nn.ParameterDict = nn.ParameterDict(\n",
    "            {\n",
    "                \"qa_st\": torch.tensor(0.0),\n",
    "                \"qb_st\": torch.tensor(0.0),\n",
    "                \"qa_ae\": torch.tensor(0.0),\n",
    "                \"qb_ae\": torch.tensor(0.0),\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def is_set(self, p_dic: nn.ParameterDict) -> bool:\n",
    "        \"\"\"Check if any of the parameters in the parameter dictionary is set.\n",
    "\n",
    "        Args:\n",
    "            p_dic (nn.ParameterDict): Parameter dictionary.\n",
    "\n",
    "        Returns:\n",
    "            bool: Boolean indicating whether any of the parameters in the parameter dictionary is set.\n",
    "        \"\"\"\n",
    "        return any(value.sum() != 0 for _, value in p_dic.items())\n",
    "\n",
    "    def choose_random_aug_image(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Choose a random augmentation function and apply it to the input image.\n",
    "\n",
    "        Args:\n",
    "            image (torch.Tensor): Input image.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Augmented image.\n",
    "        \"\"\"\n",
    "        transform_functions = [\n",
    "            transforms.functional.adjust_brightness,\n",
    "            transforms.functional.adjust_contrast,\n",
    "            transforms.functional.adjust_saturation,\n",
    "        ]\n",
    "        # Sample an augmentation coefficient Î» from the uniform distribution U(0.8, 1.2)\n",
    "        coefficient = np.random.default_rng().uniform(0.8, 1.2)\n",
    "        transform_function = np.random.default_rng().choice(transform_functions)\n",
    "        return transform_function(image, coefficient)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch: torch.Tensor,\n",
    "        batch_imagenet: torch.Tensor | None = None,\n",
    "        normalize: bool = True,\n",
    "    ) -> torch.Tensor | dict:\n",
    "        \"\"\"Perform the forward-pass of the EfficientAd models.\n",
    "\n",
    "        Args:\n",
    "            batch (torch.Tensor): Input images.\n",
    "            batch_imagenet (torch.Tensor): ImageNet batch. Defaults to None.\n",
    "            normalize (bool): Normalize anomaly maps or not\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        image_size = batch.shape[-2:]\n",
    "        with torch.no_grad():\n",
    "            teacher_output = self.teacher(batch)\n",
    "            if self.is_set(self.mean_std):\n",
    "                teacher_output = (teacher_output - self.mean_std[\"mean\"]) / self.mean_std[\"std\"]\n",
    "\n",
    "        student_output = self.student(batch)\n",
    "        distance_st = torch.pow(teacher_output - student_output[:, : self.teacher_out_channels, :, :], 2)\n",
    "\n",
    "        if self.training:\n",
    "            # Student loss\n",
    "            distance_st = reduce_tensor_elems(distance_st)\n",
    "            d_hard = torch.quantile(distance_st, 0.999)\n",
    "            loss_hard = torch.mean(distance_st[distance_st >= d_hard])\n",
    "            student_output_penalty = self.student(batch_imagenet)[:, : self.teacher_out_channels, :, :]\n",
    "            loss_penalty = torch.mean(student_output_penalty**2)\n",
    "            loss_st = loss_hard + loss_penalty\n",
    "\n",
    "            # Autoencoder and Student AE Loss\n",
    "            aug_img = self.choose_random_aug_image(batch)\n",
    "            ae_output_aug = self.ae(aug_img, image_size)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_output_aug = self.teacher(aug_img)\n",
    "                if self.is_set(self.mean_std):\n",
    "                    teacher_output_aug = (teacher_output_aug - self.mean_std[\"mean\"]) / self.mean_std[\"std\"]\n",
    "\n",
    "            student_output_ae_aug = self.student(aug_img)[:, self.teacher_out_channels :, :, :]\n",
    "\n",
    "            distance_ae = torch.pow(teacher_output_aug - ae_output_aug, 2)\n",
    "            distance_stae = torch.pow(ae_output_aug - student_output_ae_aug, 2)\n",
    "\n",
    "            loss_ae = torch.mean(distance_ae)\n",
    "            loss_stae = torch.mean(distance_stae)\n",
    "            return (loss_st, loss_ae, loss_stae)\n",
    "\n",
    "        # Eval mode.\n",
    "        with torch.no_grad():\n",
    "            ae_output = self.ae(batch, image_size)\n",
    "\n",
    "            map_st = torch.mean(distance_st, dim=1, keepdim=True)\n",
    "            map_stae = torch.mean(\n",
    "                (ae_output - student_output[:, self.teacher_out_channels :]) ** 2,\n",
    "                dim=1,\n",
    "                keepdim=True,\n",
    "            )\n",
    "\n",
    "        if self.pad_maps:\n",
    "            map_st = F.pad(map_st, (4, 4, 4, 4))\n",
    "            map_stae = F.pad(map_stae, (4, 4, 4, 4))\n",
    "        map_st = F.interpolate(map_st, size=image_size, mode=\"bilinear\")\n",
    "        map_stae = F.interpolate(map_stae, size=image_size, mode=\"bilinear\")\n",
    "\n",
    "        if self.is_set(self.quantiles) and normalize:\n",
    "            map_st = 0.1 * (map_st - self.quantiles[\"qa_st\"]) / (self.quantiles[\"qb_st\"] - self.quantiles[\"qa_st\"])\n",
    "            map_stae = 0.1 * (map_stae - self.quantiles[\"qa_ae\"]) / (self.quantiles[\"qb_ae\"] - self.quantiles[\"qa_ae\"])\n",
    "\n",
    "        map_combined = 0.5 * map_st + 0.5 * map_stae\n",
    "        return {\"anomaly_map\": map_combined, \"map_st\": map_st, \"map_ae\": map_stae}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def teacher_channel_mean_std(model, dataloader: DataLoader) -> dict[str, torch.Tensor]:\n",
    "    \"\"\"Calculate the mean and std of the teacher models activations.\n",
    "\n",
    "    Adapted from https://math.stackexchange.com/a/2148949\n",
    "\n",
    "    Args:\n",
    "        dataloader (DataLoader): Dataloader of the respective dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, torch.Tensor]: Dictionary of channel-wise mean and std\n",
    "    \"\"\"\n",
    "    arrays_defined = False\n",
    "    n: torch.Tensor | None = None\n",
    "    chanel_sum: torch.Tensor | None = None\n",
    "    chanel_sum_sqr: torch.Tensor | None = None\n",
    "\n",
    "    for batch in tqdm.tqdm(dataloader, desc=\"Calculate teacher channel mean & std\", position=0, leave=True):\n",
    "        y = model.teacher(batch.to(device))\n",
    "        if not arrays_defined:\n",
    "            _, num_channels, _, _ = y.shape\n",
    "            n = torch.zeros((num_channels,), dtype=torch.int64, device=y.device)\n",
    "            chanel_sum = torch.zeros((num_channels,), dtype=torch.float32, device=y.device)\n",
    "            chanel_sum_sqr = torch.zeros((num_channels,), dtype=torch.float32, device=y.device)\n",
    "            arrays_defined = True\n",
    "\n",
    "        n += y[:, 0].numel()\n",
    "        chanel_sum += torch.sum(y, dim=[0, 2, 3])\n",
    "        chanel_sum_sqr += torch.sum(y**2, dim=[0, 2, 3])\n",
    "\n",
    "    assert n is not None\n",
    "\n",
    "    channel_mean = chanel_sum / n\n",
    "\n",
    "    channel_std = (torch.sqrt((chanel_sum_sqr / n) - (channel_mean**2))).float()[None, :, None, None]\n",
    "    channel_mean = channel_mean.float()[None, :, None, None]\n",
    "\n",
    "    return {\"mean\": channel_mean, \"std\": channel_std}\n",
    "\n",
    "def _get_quantiles_of_maps(maps: list[torch.Tensor]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Calculate 90% and 99.5% quantiles of the given anomaly maps.\n",
    "\n",
    "    If the total number of elements in the given maps is larger than 16777216\n",
    "    the returned quantiles are computed on a random subset of the given\n",
    "    elements.\n",
    "\n",
    "    Args:\n",
    "        maps (list[torch.Tensor]): List of anomaly maps.\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: Two scalars - the 90% and the 99.5% quantile.\n",
    "    \"\"\"\n",
    "    maps_flat = reduce_tensor_elems(torch.cat(maps))\n",
    "    qa = torch.quantile(maps_flat, q=0.9).to(device)\n",
    "    qb = torch.quantile(maps_flat, q=0.995).to(device)\n",
    "    return qa, qb\n",
    "\n",
    "@torch.no_grad()\n",
    "def map_norm_quantiles(model, dataloader: DataLoader) -> dict[str, torch.Tensor]:\n",
    "    \"\"\"Calculate 90% and 99.5% quantiles of the student(st) and autoencoder(ae).\n",
    "\n",
    "    Args:\n",
    "        dataloader (DataLoader): Dataloader of the respective dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, torch.Tensor]: Dictionary of both the 90% and 99.5% quantiles\n",
    "        of both the student and autoencoder feature maps.\n",
    "    \"\"\"\n",
    "    maps_st = []\n",
    "    maps_ae = []\n",
    "    logger.info(\"Calculate Validation Dataset Quantiles\")\n",
    "    for batch in tqdm.tqdm(dataloader, desc=\"Calculate Validation Dataset Quantiles\", position=0, leave=True):\n",
    "        for img in batch:\n",
    "            output = model(img.to(device), normalize=False)\n",
    "            map_st = output[\"map_st\"]\n",
    "            map_ae = output[\"map_ae\"]\n",
    "            maps_st.append(map_st)\n",
    "            maps_ae.append(map_ae)\n",
    "                \n",
    "\n",
    "    qa_st, qb_st = _get_quantiles_of_maps(maps_st)\n",
    "    qa_ae, qb_ae = _get_quantiles_of_maps(maps_ae)\n",
    "    return {\"qa_st\": qa_st, \"qa_ae\": qa_ae, \"qb_st\": qb_st, \"qb_ae\": qb_ae}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "\n",
    "def train_model(model, train_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    data_iter = iter(imagenet_loader)\n",
    "    counter = 0\n",
    "    total_loss = 0\n",
    "    for batch_images in train_loader:\n",
    "        batch_images = batch_images.to(device)\n",
    "        imagenet_batch = next(data_iter).to(device)\n",
    "        \n",
    "        loss_st, loss_ae, loss_stae = model(batch_images, imagenet_batch)\n",
    "        \n",
    "        loss = loss_st + loss_ae + loss_stae\n",
    "        if counter % 10 == 0:\n",
    "            print(f'Batch {counter}, Training Loss: {loss.item()}')\n",
    "        counter += 1\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    print(f'Total Training Loss in Epoch: {total_loss}')\n",
    "\n",
    "def validate_model(model, validation_loader, device):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_images in validation_loader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            anomaly_maps = model(batch_images, normalize=False)[\"anomaly_map\"]\n",
    "            score = anomaly_maps.squeeze().mean(dim=(1, 2))\n",
    "            loss += score.sum().cpu().numpy()\n",
    "        print(f'Validation Loss: {loss}')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "use_pretrained = True\n",
    "pretrained_path = 'efficientad_model_medium_5_3.9978248327970505.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_epochs = 80\n",
    "learning_rate = 0.0001\n",
    "weight_decay=0.00001\n",
    "\n",
    "model_size = EfficientAdModelSize.M\n",
    "\n",
    "model = EfficientAdModel(teacher_out_channels=384, model_size=model_size, padding=False, pad_maps=True).to(device)\n",
    "optimizer = optim.Adam(list(model.student.parameters()) + list(model.ae.parameters()),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(0.95 * num_epochs * len(train_loader)), gamma=0.1)\n",
    "\n",
    "if use_pretrained:\n",
    "    model.load_state_dict(torch.load(os.path.join('results', pretrained_path), map_location=torch.device(device)))\n",
    "else:\n",
    "    teacher_path = (Path(\"./pre_trained/\") / \"efficientad_pretrained_weights\" / f\"pretrained_teacher_{model_size.value}.pth\")\n",
    "    model.teacher.load_state_dict(torch.load(teacher_path, map_location=torch.device(device)))\n",
    "    channel_mean_std = teacher_channel_mean_std(model, train_loader)\n",
    "    model.mean_std.update(channel_mean_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_validation_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    train_model(model, train_loader, optimizer, scheduler, device)\n",
    "    loss = validate_model(model, combined_validation_loader, device)\n",
    "    if loss < best_validation_loss:\n",
    "        best_validation_loss = loss\n",
    "        torch.save(model.state_dict(), f\"efficientad_model_{model_size.value}_{epoch + 1}_{loss}.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), f\"efficientad_model_{model_size.value}_{num_epochs}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def test_model(model, test_loader, device='mps'):\n",
    "    model.eval()\n",
    "    map_norm_q = map_norm_quantiles(model, validation_loader)\n",
    "    model.quantiles.update(map_norm_q)\n",
    "    y_true = []\n",
    "    y_score = []\n",
    "    with torch.no_grad():\n",
    "        for batch_images in test_loader:\n",
    "            images = batch_images['image'].to(device)\n",
    "            anomaly_maps = model(images)[\"anomaly_map\"]\n",
    "            anomalous_images = [anomaly_map.squeeze().amax(dim=(0, 1)) for anomaly_map in anomaly_maps]\n",
    "            y_true += [1 if batch_images['anomaly_type'][i] != 'good' else 0 for i in range(len(batch_images['anomaly_type']))]\n",
    "            y_score += [anomalous_images[i].item() for i in range(len(anomalous_images))]\n",
    "            # print(f'Batches done: {len(y_true)}')\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    return auc * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MVTecDataset(root_dir='mvtec_loco_anomaly_detection', category='breakfast_box', phase='test', transform=transform, anomaly_types=['logical_anomalies', 'good', 'structural_anomalies'])\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('results', 'efficientad_model_medium_2_3.864782866090536.pth')\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "accuracy = test_model(model, test_loader, device=device)\n",
    "print(f'AUC: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = os.listdir('results')\n",
    "model_list.sort()\n",
    "# model_list = model_list[:-2]\n",
    "\n",
    "for model_path_idx in range(0, len(model_list)):\n",
    "    model_path = model_list[model_path_idx]\n",
    "    try:\n",
    "        if os.path.isdir(model_path):\n",
    "            continue\n",
    "        model.load_state_dict(torch.load(os.path.join('results', model_path), map_location=torch.device(device)))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        accuracy = test_model(model, test_loader, device=device)\n",
    "        print(f'Model: {model_path}, AUC: {accuracy}')\n",
    "    except Exception as e:\n",
    "        print(f'Error in model {model_path}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test structural anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MVTecDataset(root_dir='mvtec_loco_anomaly_detection', category='screw_bag', phase='test', transform=transform, anomaly_types=['structural_anomalies', 'good'])\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('results', 'efficientad_model_medium_28_2.9415077567100525.pth')\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "accuracy = test_model(model, test_loader, device=device)\n",
    "print(f'AUC: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test logical anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MVTecDataset(root_dir='mvtec_loco_anomaly_detection', category='screw_bag', phase='test', transform=transform, anomaly_types=['logical_anomalies', 'good'])\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('results', 'efficientad_model_medium_28_2.9415077567100525.pth')\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "accuracy = test_model(model, test_loader, device=device)\n",
    "print(f'AUC: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-L/14@336px\", device=device)\n",
    "# clip_model, preprocess = clip.load(\"ViT-L/14@336px\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_train_dataset = MVTecDataset(root_dir='mvtec_loco_anomaly_detection', category='screw_bag', phase='train', transform=preprocess)\n",
    "clip_train_loader = DataLoader(dataset=clip_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "clip_validation_dataset = MVTecDataset(root_dir='mvtec_loco_anomaly_detection', category='screw_bag', phase='validation', transform=preprocess)\n",
    "clip_validation_loader = DataLoader(dataset=clip_validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "clip_test_dataset = MVTecDataset(root_dir='mvtec_loco_anomaly_detection', category='screw_bag', phase='test', transform=transform, clip_transform=preprocess)\n",
    "clip_test_loader = DataLoader(dataset=clip_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_with_validation(scores, mean_val, std_val, ratio=0.05):\n",
    "    normalized_scores = (scores - mean_val) / std_val\n",
    "    normalized_scores = np.where(normalized_scores < 0, 0, normalized_scores)\n",
    "    return normalized_scores * ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_and_cov(model, dataloader):\n",
    "    embeddings_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images = batch.to(device)\n",
    "\n",
    "            embeddings = model.encode_image(images).cpu().numpy()\n",
    "            \n",
    "            embeddings_list.extend(embeddings)\n",
    "\n",
    "            print(f'Images done: {len(embeddings_list)}')\n",
    "    \n",
    "    mean_embedding = np.mean(embeddings_list, axis=0)\n",
    "    \n",
    "    cov_embedding = np.cov(embeddings_list, rowvar=False)\n",
    "    \n",
    "    return mean_embedding, cov_embedding\n",
    "\n",
    "mean_embedding, cov_embedding = compute_mean_and_cov(clip_model, clip_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mahalanobis_distance(embedding, mean_embedding, cov_embedding):\n",
    "    diff = embedding - mean_embedding\n",
    "    epsilon = 1e-5\n",
    "    regularized_cov = cov_embedding + np.eye(cov_embedding.shape[0]) * epsilon\n",
    "    inv_cov = np.linalg.inv(regularized_cov)\n",
    "    dist = np.sqrt(np.dot(np.dot(diff, inv_cov), diff))\n",
    "    return dist\n",
    "\n",
    "def compute_distance_mean_std(model, dataloader, mean_embedding, cov_embedding):\n",
    "    distances = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images = batch.to(device)\n",
    "\n",
    "            embeddings = model.encode_image(images).cpu().numpy()\n",
    "            for embedding in embeddings:\n",
    "                dist = compute_mahalanobis_distance(embedding, mean_embedding, cov_embedding)\n",
    "                distances.append(dist)\n",
    "    \n",
    "    mean = np.mean(distances)\n",
    "    std = np.std(distances)\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "def compute_mahalanobis_distances(test_loader, mean_embedding, cov_embedding):\n",
    "    distances = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images_preprocessed = torch.stack([preprocess(Image.fromarray(img.numpy())) for img in batch]).to(device)\n",
    "            \n",
    "            embeddings = model.encode_image(images_preprocessed).cpu().numpy()\n",
    "            \n",
    "            for embedding in embeddings:\n",
    "                dist = compute_mahalanobis_distance(embedding, mean_embedding, cov_embedding)\n",
    "                distances.append(dist)\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLIP_method1_compute_parameters(model, validation_loader, train_loader):\n",
    "    mean_embedding, cov_embedding = compute_mean_and_cov(model, train_loader)\n",
    "    return compute_distance_mean_std(model, validation_loader, mean_embedding, cov_embedding)\n",
    "\n",
    "def CLIP_method1(model, clip_images, parameters):\n",
    "    distances_mean, distances_std = parameters\n",
    "    embeddings = model.encode_image(clip_images).cpu().numpy()\n",
    "    distances = [compute_mahalanobis_distance(embedding, mean_embedding, cov_embedding) for embedding in embeddings]\n",
    "    normalized_distances = normalize_with_validation(distances, distances_mean, distances_std)\n",
    "    return normalized_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def CLIP_method2_compute_parameters(model, train_loader, validation_loader, optimal_n_components=1):\n",
    "    train_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:\n",
    "            images = batch.to(device)\n",
    "            embeddings = model.encode_image(images).cpu().numpy()\n",
    "            train_embeddings.extend(embeddings)\n",
    "    validation_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_loader:\n",
    "            images = batch.to(device)\n",
    "            embeddings = model.encode_image(images).cpu().numpy()\n",
    "            validation_embeddings.extend(embeddings)\n",
    "\n",
    "    gmm = GaussianMixture(n_components=optimal_n_components)\n",
    "    gmm.fit(train_embeddings)\n",
    "\n",
    "    anomaly_scores = -gmm.score_samples(validation_embeddings)\n",
    "    \n",
    "    std = np.std(anomaly_scores)\n",
    "    mean = np.mean(anomaly_scores)\n",
    "\n",
    "    return gmm, std, mean\n",
    "\n",
    "def CLIP_method2(model, images, parameters, ratio = 0.05):\n",
    "    gmm, std, mean = parameters\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.encode_image(images).cpu().numpy()\n",
    "    anomaly_scores = -gmm.score_samples(embeddings)\n",
    "    normalized_scores = normalize_with_validation(anomaly_scores, mean, std, ratio)\n",
    "    return normalized_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def test_clip_model(clip_model, test_loader, device='mps'):\n",
    "    parameters = CLIP_method2_compute_parameters(clip_model, clip_train_loader, clip_validation_loader)\n",
    "    y_true = []\n",
    "    y_score = []\n",
    "    with torch.no_grad():\n",
    "        for batch_images in test_loader:\n",
    "            clip_images = batch_images['clip_image'].to(device)\n",
    "            clip_scores = CLIP_method2(clip_model, clip_images, parameters)\n",
    "            print(f\"Anomality scores CLIP: {clip_scores}\")\n",
    "            print(f'Anomaly type: {batch_images[\"anomaly_type\"]}')\n",
    "\n",
    "            y_true += [1 if batch_images['anomaly_type'][i] != 'good' else 0 for i in range(len(batch_images['anomaly_type']))]\n",
    "            print(f\"y_true: {[1 if batch_images['anomaly_type'][i] != 'good' else 0 for i in range(len(batch_images['anomaly_type']))]}\")\n",
    "            \n",
    "            y_score += list(clip_scores)\n",
    "            \n",
    "            print(f'Batches done: {len(y_true)}\\n\\n')\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    return auc * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = test_clip_model(clip_model, clip_test_loader, device=device)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test full architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run after dataset changes\n",
    "\n",
    "map_norm_q = map_norm_quantiles(model, validation_loader)\n",
    "model.quantiles.update(map_norm_q)\n",
    "parameters = CLIP_method2_compute_parameters(clip_model, clip_train_loader, clip_validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def test_full_model(efficient_model, clip_model, test_loader, device='mps', ratio = 0.05, verbose=True):\n",
    "    efficient_model.eval()\n",
    "    y_true = []\n",
    "    y_score = []\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_images in test_loader:\n",
    "            counter += 1\n",
    "            images = batch_images['image'].to(device)\n",
    "            anomaly_maps = efficient_model(images)[\"anomaly_map\"]\n",
    "            anomaly_scores_EAD = [anomaly_map.squeeze().amax(dim=(0, 1)).item() for anomaly_map in anomaly_maps]\n",
    "            anomaly_scores_EAD = [anomaly_scores_EAD[i] if anomaly_scores_EAD[i] > 0 else 0 for i in range(len(anomaly_scores_EAD))]\n",
    "\n",
    "            clip_images = batch_images['clip_image'].to(device)\n",
    "            clip_scores = CLIP_method2(clip_model, clip_images, parameters, ratio=ratio)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Anomality scores EffecientAD: {anomaly_scores_EAD}\")\n",
    "                print(f\"Anomality scores CLIP: {clip_scores}\")\n",
    "                print(f'Anomaly type: {batch_images[\"anomaly_type\"]}')\n",
    "                print(f\"y_true: {[1 if batch_images['anomaly_type'][i] != 'good' else 0 for i in range(len(batch_images['anomaly_type']))]}\")\n",
    "                print(f'Batches done: {len(y_true)}\\n\\n')\n",
    "\n",
    "            y_true += [1 if batch_images['anomaly_type'][i] != 'good' else 0 for i in range(len(batch_images['anomaly_type']))]\n",
    "            y_score += [anomaly_scores_EAD[i] + clip_scores[i] for i in range(len(anomaly_scores_EAD))]\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    return auc * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MVTecDataset(root_dir='mvtec_loco_anomaly_detection', category='screw_bag', phase='test', transform=transform, anomaly_types=['logical_anomalies', 'good', 'structural_anomalies'], clip_transform=preprocess)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = EfficientAdModelSize.M\n",
    "\n",
    "model = EfficientAdModel(teacher_out_channels=384, model_size=model_size, padding=False, pad_maps=True).to(device)\n",
    "model.load_state_dict(torch.load(os.path.join('results', 'screw_bag', 'efficientad_model_medium_screw_bag.pth'), map_location=torch.device(device)))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = test_full_model(model, clip_model, test_loader, device=device)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ratio in [0.03, 0.04, 0.5, 0.06, 0.07, 0.08, 0.09, 0.1]:\n",
    "    accuracy = test_full_model(model, clip_model, test_loader, device=device, ratio=ratio, verbose=False)\n",
    "    print(f'Ratio: {ratio}, AUC: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MVTecDataset(root_dir='mvtec_loco_anomaly_detection', category='screw_bag', phase='test', transform=transform, anomaly_types=['good', 'logical_anomalies'], clip_transform=preprocess)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_norm_q = map_norm_quantiles(model, validation_loader)\n",
    "model.quantiles.update(map_norm_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "img_idx = 331\n",
    "\n",
    "dataset = test_dataset\n",
    "\n",
    "image = dataset[img_idx]['image']\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(f\"Image path {dataset.data[img_idx]}\")\n",
    "    img = torch.unsqueeze(image, 0).to(device)\n",
    "    outputs = model(img)\n",
    "    map = outputs['anomaly_map']\n",
    "    map_st = outputs['map_st']\n",
    "    map_ae = outputs['map_ae']\n",
    "    map = map.squeeze().cpu().numpy()\n",
    "    map_st = map_st.squeeze().cpu().numpy()\n",
    "    map_ae = map_ae.squeeze().cpu().numpy()\n",
    "\n",
    "map = np.where(map < 0.10, 0, map)\n",
    "map_st = np.where(map_st < 0.10, 0, map_st)\n",
    "map_ae = np.where(map < 0.10, 0, map_ae)\n",
    "\n",
    "print(np.max(map), np.min(map))\n",
    "print(np.max(map_st), np.min(map_st))\n",
    "print(np.max(map_ae), np.min(map_ae))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "ax.imshow(image.permute(1, 2, 0))\n",
    "ax.imshow(map, cmap='hot', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "ax.imshow(image.permute(1, 2, 0))\n",
    "ax.imshow(map_st, cmap='hot', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "ax.imshow(image.permute(1, 2, 0))\n",
    "ax.imshow(map_ae, cmap='hot', alpha=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
